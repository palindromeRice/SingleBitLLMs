# ğŸ§  Single-Bit Quantization in NLP: Experiments & Insights

## ğŸ“Œ Overview
Microsoftâ€™s **BitNet b1.58**â€ƒâ€”â€‚a *1.58-bit* large-language model (LLM) capable of running on commodity CPUs â€” reignited interest in ultra-low-precision inference.  
Inspired by this work, I explored **single-bit (and ternary) quantization** on the SST-2 sentiment-analysis task.  
This repo walks through **eight progressively refined approaches**, starting from scratch-built transformers and culminating in a quantized, fine-tuned BERT.

---

## ğŸ§ª Experimentation Strategy
1. **Baseline** â€“ scratch-built transformer + 1-bit weights.  
2. **Incremental tricks** â€“ add positional encoding, dropout, mixed precision, QAT.  
3. **Advanced tricks** â€“ median-scaling, Straight-Through Estimator (STE), progressive Mixed-precision Quantization (MoQ).  
4. **Pre-trained models** â€“ swap in BERT, then apply STE / ternary + activation quantization.

At each stage I addressed shortcomings of the previous approach while monitoring accuracy/F1, model size, and training stability.

---

## ğŸ” Detailed Approaches

<details>
<summary><strong>Approach 1 â€“ Simple Quantized Transformer (Classifier)</strong></summary>

* **Goal** â€“ prove 1-bit feasibility.  
* **Key steps**
  * Scratch implementation of a miniature Transformer encoder.
  * Replaced all linear layers with custom <code>BitLinear</code> (sign-only weights).
  * Adam + CE loss; no fancy schedulers.
* **Results** â€“ <br>Accuracy 76.38 %â€ƒ|â€ƒF1 76.38 %  
* **Takeaway** â€“ works, but capacity is tiny and no positional clues â†’ limited ceiling.
</details>

<details>
<summary><strong>Approach 2 â€“ + Positional Encoding & Mixed Precision</strong></summary>

* Added sinusoidal PE, automatic mixed precision (AMP), scheduler + grad-clip.  
* **Results** â€“ 62.27 % / 61.26 %.  
* **Why worse?** AMP introduced instability with sign-only weights; capacity still low.
</details>

<details>
<summary><strong>Approach 3 â€“ + Dropout & Quantization-Aware Training (QAT)</strong></summary>

* Injected dropout; trained with fake-quant ops (PyTorch QAT).  
* **Results** â€“ 63.88 % / 63.65 %.  
* **Takeaway** â€“ tiny bump; still under-fits.
</details>

<details>
<summary><strong>Approach 4 â€“ Median Scaling + Straight-Through Estimator (STE)</strong></summary>

* Normalised activations via median scaling; back-prop with STE.  
* **Results** â€“ 69.84 % / 69.65 %.  
* **Takeaway** â€“ big jump â†’ scaling + STE help gradients flow in 1-bit nets.
</details>

<details>
<summary><strong>Approach 5 â€“ Variant of (4)</strong></summary>

* Tweaked scaling factor & clipping range.  
* **Results** â€“ 70.76 % / 70.66 %.  
* **Takeaway** â€“ careful hyper-tuning matters even in low-bit land.
</details>

<details>
<summary><strong>Approach 6 â€“ Multi-Head Attention & Progressive MoQ</strong></summary>

* Upgraded to full MH-Attention encoder; progressively lowered precision (8â†’4â†’1-bit) during fine-tuning.  
* **Results** â€“ 70.18 % / 70.15 %.  
* **Takeaway** â€“ capacity â†‘, but extra heads partly cancelled by quantization loss.
</details>

<details>
<summary><strong>Approach 7 â€“ Pre-trained BERT (+ STE)</strong></summary>

* Started from <code>bert-base-uncased</code>; swapped every dense/attn projection to BitLinear; STE for back-prop.  
* **Results** â€“ **85.67 % / 85.65 %** (best).  
* **Takeaway** â€“ pre-training supplies strong linguistic priors; 1-bit layers fine-tune well with STE.
</details>

<details>
<summary><strong>Approach 8 â€“ Ternary BERT (+ Activation Quant)</strong></summary>

* Pushed further: ternary weights {-1,0,+1} + per-layer activation quant + sub-layer norm.  
* **Results** â€“ 50.92 % / 34.36 %.  
* **Takeaway** â€“ too aggressive; activation quant hurt expressive power.
</details>

---

## ğŸ“Š Result Table

| # | Model / Technique                                                                    | Acc. | F1  |
|:-:|---------------------------------------------------------------------------------------|:---:|:---:|
| 1 | Scratch Transformer + 1-bit weights                                                  | 76.38 | 76.38 |
| 2 | + PosEnc & AMP                                                                        | 62.27 | 61.26 |
| 3 | + Dropout & QAT                                                                       | 63.88 | 63.65 |
| 4 | + Median Scaling & STE                                                                | 69.84 | 69.65 |
| 5 | Variant of 4                                                                          | 70.76 | 70.66 |
| 6 | + MH-Attention & Progressive MoQ                                                     | 70.18 | 70.15 |
| 7 | **BERT-base + STE-quantized**                                                         | **85.67** | **85.65** |
| 8 | Ternary BERT (+ Activation Quant)                                                     | 50.92 | 34.36 |




